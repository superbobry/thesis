%% -*- TeX-engine: xetex; TeX-master: "draft.tex"; ispell-dictionary: "russian" -*-

\section{Предлагаемые модели}
\label{sec:models}

\subsection{Конкретизация задачи}

Пусть $\mathbf{x} = (x_1, \ldots, x_T)$ --- некоторое представление результатов
бисульфитного секвенирования, где $t$-му цитозину соответствует наблюдение $x_t$.
Мы будем предполагать, что каждому наблюдению $x_t$ можно сопоставить некоторую
метку-состояние $s_t$ из множества состояний $\{1, \ldots, S\}$. Истинные значения
$s_t$ нам не известны. Вероятностная модель позволяет найти наиболее правдоподобную
(в смысле модели) последовательность состояний:
$$
\hat{\mathbf{s}}_{ML}
= \argmax\limits_{\mathbf{s} \in \{1, \ldots, S\}^T} P(\mathbf{x}, \mathbf{s}|\mathbf{\theta}),
$$
где $\mathbf{\theta}$ --- вектор параметров модели.

В данной работе используется множество состояний, предложенное в \cite{Stadler2011}:
состояние $1$ или \texttt{UNMETHYLATED} соответствует ошибкам бисульфитной конверсии,
$2$ или \texttt{LOW} --- областям гипометилирования, а $3$ или \texttt{HIGH} ---
областям гиперметилирования.

\subsection{Биномиальная смесь}

\subsubsection{Мотивация}

Пусть $x_t = (k_t, n_t)$ --- количество прочтений, подтверждающих метилирование,
и общее количество прочтений для $t$-го цитозина. При построении самой простой модели
предполагается, что состояния всех наблюдений независимы и одинаково распределены,
а также, что $k_t$ подчиняются биномиальному распределению с параметрами $n_t$ и $p_{s_t}$,
где $p_{s_t}$ --- вероятность того, что прочтение подтверждает метилирование цитозина.
Такое предположение соответствует смеси $S$ биномиальных распределений. Распределения,
образующие смесь, называются \emph{компонентами} смеси.

\subsubsection{Определение}
Пусть $\mathbf{\pi} = (\pi_1, \ldots, \pi_S)$ --- априорные вероятности компонент смеси
и $\mathbf{p} = (p_1, \ldots, p_S)$ --- параметры биномиального распределения для каждой
из компонент, причём
$$
\sum\limits_{i = 1}^S \pi_i = 1,
$$
тогда правдоподобие данных при условии модели
$$
P(\mathbf{k}; \mathbf{n}, \mathbf{\pi}, \mathbf{p})
= \prod\limits_{t = 1}^T \sum\limits_{i = 1}^S
  \pi_i \op{Bin}(k_t; n_t, p_i)
$$

Для модели можно также сформулировать порождающий процесс, то есть описание того,
как с её помощью можно получить наблюдаемую выборку: для каждого $t$-го наблюдения
выберем скрытое состояние $s_t$ из дискретного распределения с параметрами $\mathbf{\pi}$,
а затем породим $k_t$ из биномиального распределения с параметрами $n_t$ и $p_{s_t}$
(рис.~\ref{fig:generative-mm}).

\begin{figure}[h!]
  \centering
  \begin{tikzpicture}
    \node[latent]              (pi) {$\pi_i$};
    \node[latent, left=of pi] (z)  {$s_t$};
    \node[latent, below=of pi] (p)  {$p_i$};
    \node[obs, below=of z]     (k)  {$k_t$};
    \node[obs, left=of k]     (n)  {$n_t$};

    \edge {pi} {z};
    \edge {n,p,z} {k};

    \plate {pip} {(pi)(p)} {$S$};
    \plate {knz} {(k)(n)(z)} {$T$};
  \end{tikzpicture}
  \caption{Графическая диаграмма для биномиальной смеси}
  \label{fig:generative-mm}
\end{figure}

\subsubsection{ОМП-вывод}

Найти точное решение задачи ОМП (оценки максимального правдоподобия) для параметров биномиальной
смеси довольно сложно, так как для этого требуется оптимизировать сумму по всем $S^T$ возможным
последовательностям скрытых состояний $\mathbf{s}$.

\begin{align*}
  \widehat{\theta}_{ML}
  = \argmax_{\mathbf{\theta} \in \Theta}
    P(\mathbf{k}; \mathbf{n}, \mathbf{\theta})
  = \argmax_{\mathbf{\theta} \in \Theta}
    \sum\limits_{\mathbf{s} \in \{1, \ldots, S\}^T}
    P(\mathbf{k}, \mathbf{s}; \mathbf{n}, \mathbf{\theta})
\end{align*}

Стандартным способом поиска приближенного решения является EM\hyp алгоритм \cite{dempster1977maximum},
который итеративно оптимизирует нижнюю оценку на правдоподобие до сходимости к локальному
максимуму. В общем виде EM\hyp алгоритм формулируется следующим образом
\cite[С. 351-352]{murphy2012machine}.

\begin{enumerate}
\item Выбрать начальные значения параметров $\mathbf{\theta}^{\op{old}}$.
\item Вычислить нижнюю оценку на правдоподобие:
  $$
  Q(\mathbf{\theta}|\mathbf{\theta}^{\op{old}})
  := \E{\mathbf{s}|\mathbf{k}, \mathbf{n}, \mathbf{\theta^{\op{old}}}}
       {\log P(\mathbf{k}, \mathbf{s}; \mathbf{n}, \mathbf{\theta})}
  \le \log P(\mathbf{k}; \mathbf{n}, \mathbf{\theta})
  $$

\item Найти новое приближение для параметров:
  $$
  \mathbf{\theta}^{\op{new}} := \argmax_{\mathbf{\theta} \in \Theta}
      Q(\mathbf{\theta}|\mathbf{\theta}^{\op{old}})
  $$
\item Если некоторый критерий сходимости не удовлетворён, то заменить
  $\mathbf{\theta}^{\op{old}} = \mathbf{\theta}^{\op{new}}$ и повторить с шага 2.
\end{enumerate}

Сходимость EM\hyp алгоритма к глобальному максимуму не гарантируется, но можно показать,
что каждая итерация алгоритма не уменьшает правдоподобие \cite[С. 366-367]{murphy2012machine}.

Выведем EM-алгоритм для биномиальной смеси, он состоит из двух шагов: шаг E и шаг M.

\paragraph{Шаг E}

\begin{align*}
  Q(\mathbf{\theta}|\mathbf{\theta}^{\op{old}})
  &= \E{\mathbf{s}|\mathbf{k}, \mathbf{n}, \mathbf{\theta^{\op{old}}}}
       {\log P(\mathbf{k}, \mathbf{s}; \mathbf{\theta})} \\
  &= \E{\mathbf{s}|\mathbf{k}, \mathbf{n}, \mathbf{\theta^{\op{old}}}}
       {\sum\limits_{t = 1}^T \log \sum\limits_{i = 1}^S
            \left( \pi_i \op{Bin}(k_t; n_t, p_i) \right)^{\I{s_t=i}}} \\
  &= \E{\mathbf{s}|\mathbf{k}, \mathbf{n}, \mathbf{\theta^{\op{old}}}}
       {\sum\limits_{t = 1}^T \sum\limits_{i = 1}^S
            \I{s_t=i} \left( \log \pi_i + \log \op{Bin}(k_t; n_t, p_i) \right)} \\
  &= \sum\limits_{t = 1}^T \sum\limits_{i = 1}^S
     P(s_t = i|\mathbf{k}, \mathbf{n}; \mathbf{\theta}^{\op{old}})
     \left( \log \pi_i + \log \op{Bin}(k_t; n_t, p_i) \right) \\
  &= \sum\limits_{t = 1}^T \sum\limits_{i = 1}^S
     \gamma_{ti} \left( \log \pi_i + \log \op{Bin}(k_t; n_t, p_i) \right), \\
\end{align*}
где
$$
\gamma_{ti}
= P(s_t = i|\mathbf{k}, \mathbf{n}; \mathbf{\theta}^{\op{old}})
= \frac{\pi_i \op{Bin}(k_t; n_t, p_i)}
       {\sum\limits_{i' = 1}^S \pi_{i'} \op{Bin}(k_t; n_t, p_{i'})}.
$$

\paragraph{Шаг M}

Для нахождения нового приближения возьмём частные производные
$Q(\mathbf{\theta}|\mathbf{\theta}^{\op{old}})$ по параметрам.

\begin{align*}
  \frac{\partial}{\partial p_i} Q(\mathbf{\theta}|\mathbf{\theta}^{\op{old}})
  &= \sum\limits_{t = 1}^T \gamma_{ti} \left(
          \frac{k_t}{p_i} + \frac{n_t - k_t}{p_i - 1}
     \right) \\
  &= \frac{1}{p_i} \sum\limits_{t = 1}^T \gamma_{ti} k_t
   + \frac{1}{p_i - 1} \sum\limits_{t = 1}^T \gamma_{ti} (n_t - k_t) \equiv 0 \\
  &\Leftrightarrow
  p_i^{\op{new}} = \frac{\sum\limits_{t = 1}^T \gamma_{ti} k_t}{\sum\limits_{t = 1}^T \gamma_{ti} n_t}
\end{align*}

Воспользуемся методом множителей Лагранжа для учёта ограничений на вектор
априорных вероятностей $\mathbf{\pi}$. Для этого составим функцию Лагранжа
$$
L(\mathbf{\theta}, \lambda|\mathbf{\theta}^{\op{old}})
= Q(\mathbf{\theta}|\mathbf{\theta}^{\op{old}})
+ \lambda (1 - \sum\limits_{i = 1}^S \pi_i),
$$
где $\lambda$ --- множитель Лагранжа, и возьмём её частные производные по $\pi_i$
и $\lambda$.

\begin{align*}
  \frac{\partial}{\partial \pi_i} L(\mathbf{\theta}, \lambda|\mathbf{\theta}^{\op{old}})
  &= \sum\limits_{t = 1}^T \gamma_{ti} - \lambda \pi_i \equiv 0
     \Leftrightarrow \pi_i = \frac{1}{\lambda} \sum\limits_{t = 1}^T \gamma_{ti} \\
  \frac{\partial}{\partial \lambda} L(\mathbf{\theta}, \lambda|\mathbf{\theta}^{\op{old}})
  &= 1 - \sum\limits_{i = 1}^S \pi_i \equiv 0
     \Leftrightarrow \lambda = \sum\limits_{i = 1}^S \sum\limits_{t = 1}^T\gamma_{ti}
     = T \\
  &\Rightarrow \pi_i^{\op{new}} = \frac{1}T\sum\limits_{t = 1}^T \gamma_{ti}
\end{align*}

\paragraph{ОМП для последовательности состояний}

Состояния всех наблюдений независимы. Это значит, что для нахождения наиболее правдоподобной
последовательности скрытых состояний достаточно выбрать наблюдения состояние
с наибольшей апостериорной вероятностью для каждого наблюдения:
$$
s_t
= \argmax\limits_{i \in \{1, \ldots, S\}}
  P(s_t = i|\mathbf{k}, \mathbf{n}, \mathbf{\theta})
= \argmax\limits_{i \in \{1, \ldots, S\}} \gamma_{ti}.
$$

\subsubsection{Применение}
\label{bm:applications}

Важным аспектом практического применения EM-алгоритма является нахождение начального
приближения для параметров $\mathbf{\theta}^{\op{old}}$. Инициализация биномиальной смеси
осуществляется с помощью алгоритма кластеризации KMeans++ \cite[с. 357]{murphy2012machine},
разбивающего пары $(k_t, n_t)$ на $S$ кластеров. В качестве метрики расстояния в алгоритме
KMeans++ используется Евклидова метрика. Затем для каждого кластера методом максимального
правдоподобия оценивается параметр $p$ биномиального распределения. EM-алгоритм
с полученными параметрами запускается и итерируется до тех пор, пока изменение логарифма
правдоподобия не станет меньше $10^{-3}$.

Для проверки предположения о том, что результаты бисульфитного секвенирования не однородны,
мы сравниваем биномиальную смесь с одним биномиальным распределением на данных бисульфитного
секвенирования первой хромосомы стволовых клеток мыши (см.~\ref{sub:data}). Из графика
(рис.~\ref{fig:bd-bm}) видно, что биномиальная смесь правдоподобней одного биномиального
распределения. Это подтверждает предположение о неоднородности результатов бисульфитного
секвенирования.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{images/bd-bm}
  \caption{Сравнение одного биномиального распределения (BD) и биномиальной смеси (BM)
    на данных бисульфитного секвенирования стволовых клеток мыши.}
  \label{fig:bd-bm}
\end{figure}


\subsection{Биномиальная скрытая марковская модель}

\subsubsection{Мотивация}

Предположение о независимости состояний соседних цитозинов в геноме выглядит
не очень правдоподобным \cite{Saxonov2006}. Чтобы учесть зависимость состояний двух последовательных
цитозинов, перейдём к биномиальной скрытой марковской модели (СММ) первого порядка.
В СММ состояние $t$-го цитозина зависит только от состояния $(t-1)$-го.

\subsubsection{Определение}

Пусть $\mathbf{\pi} = (\pi_1, \ldots, \pi_S)$ --- вектор априорных
вероятностей состояний, $\mathbf{A}$~---~матрица вероятностей перехода между
состояниями и $\mathbf{p} = (p_1, \ldots, p_S)$ --- параметры биномиального
распределения для каждого из состояний, причём
$$
\sum\limits_{i = 1}^S \pi_i = 1,
\qquad
\forall i \in \{1, \ldots, S\}
\left( \sum\limits_{j = 1}^S \mathbf{A}_{ij} = 1  \right),
$$
тогда функция правдоподобия определяется как
$$
P(\mathbf{k}; \mathbf{n}, \mathbf{\pi}, \mathbf{A}, \mathbf{p})
= \sum\limits_{\mathbf{s} \in \{1, \ldots, S\}^T}
  \pi_{s_1}
  \prod\limits_{t = 2}^T A_{s_t s_{t - 1}}
  \prod\limits_{t = 1}^T \op{Bin}(k_t; n_t, p_{s_t})
$$

Порождающий процесс для биномиальной скрытой марковской модели формулируется
следующим образом: случайно выберем скрытое состояние $s_1$ из дискретного
распределения с параметрами $\mathbf{\pi}$, породим $k_1$ из биномиального
распределения с параметрами $n_1$ и $p_{s_1}$. Выберем следующее состояние
$s_2$ из дискретного распределения $\mathbf{A}_{s_1}$ и продолжим аналогично
(рис.~\ref{fig:generative-hmm}).

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \node[latent] (z)  {$s_t$};
    \node[obs, below=of z]    (k)  {$k_t$};
    \node[obs, left=of k]     (n)  {$n_t$};
    \node[latent, right=of k] (p) {$p_i$};
    \node[latent, right=of z] (A) {$\mathbf{A}_i$};

    \edge {z,p,n} {k};

    \node[latent, left=of z] (z0) {$s_{t - 1}$};
    \edge {z0,A} {z};

    \plate {Ap} {(A)(p)} {$S$};
  \end{tikzpicture}
  \caption{Графическая диаграмма для биномиальной СММ}
  \label{fig:generative-hmm}
\end{figure}


\subsubsection{ОМП-вывод}

У задачи ОМП для скрытой марковской модели есть проблемы, аналогичные проблемам описанной ранее
смеси. Найти приближенное решение можно с помощью частного случая EM-алгоритма для скрытых
марковских моделей, традиционно называемого алгоритмом Баума-Велша.

\paragraph{Шаг E}

Шаг E для скрытой марковской модели предполагает вычисление:
\begin{align*}
  \E{\mathbf{s}|\mathbf{k}, \mathbf{n}, \mathbf{\theta^{\op{old}}}}{\I{s_t = i}}
  := \gamma_{ti}
  &= \frac{\alpha_i(t) \beta_i(t)}{\sum\limits_{i' = 1}^S \alpha_{i'}(t) \beta_{i'}(t)} \\
  \E{\mathbf{s}|\mathbf{k}, \mathbf{n}, \mathbf{\theta^{\op{old}}}}{\I{s_t = j} \I{s_{t - 1} = i}}
  := \xi_{tij}
  &= \frac{\alpha_i(t - 1) \mathbf{A}_{ij} \op{Bin}(k_t; n_t, p_j) \beta_j(t)}
          {\sum\limits_{i' = 1}^S \sum\limits_{j' = 1}^S
           \alpha_{i'}(t - 1) \mathbf{A}_{i'j'} \op{Bin}(k_t; n_t, p_{j'}) \beta_{j'}(t)},
\end{align*}
где величины $\alpha_i(t) = P(s_t = i, k_1, \ldots, k_t; \mathbf{\theta})$ и
$\beta_i(t) = P(k_{t + 1}, \ldots, k_T| s_t = i; \mathbf{\theta})$
находятся с помощью алгоритма прямого-обратного хода, который заключается в
динамическом вычислении следующих соотношений (см., например, \cite{Rabiner1989}):
\begin{align*}
  \alpha_i(1)
  &= \pi_i \op{Bin}(k_1; n_1, p_i) \\
  \alpha_i(t)
  &= \left( \sum\limits_{j = 1}^S \alpha_j(t - 1) \mathbf{A}_{ji}
     \right) \op{Bin}(k_t; n_t, p_i) \\
  \beta_i(T)
  &= 1 \\
  \beta_i(t)
  &= \sum\limits_{j = 1}^S \mathbf{A}_{ij} \op{Bin}(k_{t + 1}; n_{t + 1}, p_j) \beta_j(t + 1).
\end{align*}

\paragraph{Шаг M}

Новое приближение параметров находится по формулам:
\begin{align*}
  \pi_i^{\op{new}} &= \gamma_{1i} \\
  \mathbf{A}_{ij}^{\op{new}}
  &= \frac{\sum\limits_{t = 1}^T \xi_{tij}}
          {\sum\limits_{j' = 1}^S\sum\limits_{t = 1}^T \xi_{tij'}} \\
  p_i^{\op{new}}
  &= \frac{\sum\limits_{t = 1}^T \gamma_{ti} k_t}{\sum\limits_{t = 1}^T \gamma_{ti} n_t}
\end{align*}


\paragraph{Алгоритм Витерби}

Для нахождения оценки максимального правдоподобия для последовательности
скрытых состояний модели используется алгоритм Витерби \cite{Rabiner1989}. В процессе работы
алгоритм динамически максимизирует вероятность подпоследовательности
$s_1, \ldots, s_t$ скрытых состояний:
\begin{align*}
  \mathbf{V}_{ti}
  &= \max\limits_{s_1, \ldots, s_{t- 1}} P(s_t = i, s_1, \ldots, s_{t - 1}| \mathbf{k}) \\
  &= \op{Bin}(k_t; n_t, p_i) \left(
         \max\limits_{j = 1}^S \mathbf{V}_{(t - 1)j} \mathbf{A}_{ji}
     \right)
\end{align*}

Наиболее вероятная последовательность состояний может быть найдена
обратным обходом матрицы $\mathbf{V}$, начиная с позиции
$\argmax\limits_{i \in \{1, \ldots, S\}} \mathbf{V}_{T i}$.


\subsubsection{Применение}
\label{bhmm:applications}

Биномиальная скрытая марковская модель обучается на данных бисульфитного секвенирования
первой хромосомы стволовых клеток мыши (см.~\ref{sub:data}). Инициализация параметров модели
производится с помощью биномиальной смеси, обученной на тех же данных. Из графика
(рис.~\ref{fig:bm-bhmm}) можно видеть, что предположение о зависимости состояний последовательных
цитозинов подтвердилось: биномиальная скрытая марковская модель более правдоподобна и имеет
меньшее значение критерия Акаике чем биномиальная смесь.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{images/bm-bhmm}
  \caption{Сравнение биномиальной смеси (BM) и биномиальной скрытой марковской модели (BHMM)
    на данных бисульфитного секвенирования стволовых клеток мыши.}
  \label{fig:bm-bhmm}
\end{figure}


\subsection{Переключающаяся биномиальная СММ}

\subsubsection{Мотивация}

Два последовательных цитозина в моделируемых данных в геноме могут находиться
на некотором расстоянии. Предположим, что распределение состояний для следующего
цитозина зависит не только от состояния предыдущего, но также и от расстояния между
ними.

\subsubsection{Определение}

Пусть $d_t$ --- количество нуклеотидов в геноме между $(t-1)$-ым и $t$-ым цитозином,
тогда вероятность перехода из состояния $i$ в состояние $j$ на шаге $t$ можно
определить как
$$
P(s_t = j|s_{t-1} = i, d_t) \propto 1 + \left( \mathbf{A}_{ij} \right)^{d_t}.
$$

К сожалению, при таком определении вероятности перехода получить аналитическое
выражение для $\mathbf{A}_{ij}^{\op{new}}$ не представляется возможным, поэтому
стоит упростить определение, разрешая модели выбирать матрицу вероятностей
перехода в зависимости от расстояния $d_t$:
$$
P(s_t = j|s_{t-1} = i, d_t) = \mathbf{A}_{\op{w}(d_t)ij},
$$
где $\op{w}(d_t) : \mathbb{N}_0 \to \{1, \ldots, D\}$ --- функция,
кластеризующая расстояния. Необходимость в кластеризации расстояний объясняется тем, что
количество различных расстояний в моделируемых данных действительно велико
(рис.~\ref{fig:cytosine-distance}).

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{images/distance_distribution}
  \caption{Гистограмма расстояний между последовательными цитозинами на положительной
    цепи хромосомы 1 генома мыши mm9. Из соображений наглядности на гистограмме изображены
    только расстояния, встречающиеся не менее 1000 раз.}
  \label{fig:cytosine-distance}
\end{figure}

Таким образом, переключающаяся скрытая марковская модель имеет те же параметры, что
и классическая скрытая марковская модель, за исключением матрицы вероятностей перехода
между состояниями модели, которая в случае переключающейся модели индексируется
множеством расстояний $\{1, \ldots, D\}$. Запишем функцию правдоподобия:

$$
P(\mathbf{k}; \mathbf{n}, \mathbf{\pi}, \mathbf{A}, \mathbf{p})
= \sum\limits_{\mathbf{s} \in \{1, \ldots, S\}^T}
  \pi_{s_t}
  \prod\limits_{t = 2}^T A_{\op{w}(d_t) s_t s_{t - 1}}
  \prod\limits_{t = 1}^T \op{Bin}(k_t; n_t, p_{s_t}).
$$

Порождающий процесс для биномиальной переключающейся СММ эквивалентен порождающему
процессу для классической СММ, так как модель не рассматривает расстояния между
наблюдениями как случайную величину.

\subsubsection{Одномерная кластеризация расстояний}

Задача одномерной кластеризации на $D$ классов эквивалентна задаче
разбиения числовой прямой с помощью $D - 1$ разделителей. Существует
несколько подходов к расстановке разделителей, например,

\begin{itemize}
\item в качестве $i$-го разделителя можно взять $^i/_D$-ю
  выборочную квантиль\footnote{Квантилью уровня $\alpha$ распределения
    некоторой случайной величины $X$ называется такое значение $x_{\alpha}$,
    что $P(X \le x_{\alpha}) \ge \alpha$ и $P(X > x_{\alpha}) \le \alpha$.},
  или
\item воспользоваться оптимизацией естественных разделителей
  Дженкса\footnote{\url{http://en.wikipedia.org/wiki/Jenks_natural_breaks_optimization}},
  которая выбирает разделители, минимизирующие дисперсию внутри кластеров
  и максимизирующие дисперсию между кластерами.
\end{itemize}

Оба подхода требуют на вход фиксированное число кластеров и плохо работают
в случае, когда распределение данных имеет тяжелый хвост \cite{jiang2013head}.

Алгоритм <<голова/хвост>> был предложен в качестве альтернативного подхода к
расстановке нефиксированного числа разделителей специально для распределений
с тяжёлым хвостом.

\begin{enumerate}
\item Пусть $\mathbf{d} = (d_1, \ldots, d_T)$ --- вектор расстояний,
  обозначим его среднее за $\overline{d}$ и назовём головой элементы
  $\op{hd}(\mathbf{d}) = (d_t < \overline{d} | t \in \{1, \ldots, T\})$,
  а хвостом $\op{tl}(\mathbf{d}) = (d_t > \overline{d}| t \in \{1, \ldots, T\})$.
\item Добавим $\overline{d}$ во множество разделителей и заменим вектор расстояний
  на голову $\mathbf{d} = \op{hd}(\mathbf{d})$.
\item Если в новом векторе расстояний больше одного элемента, перейдем к шагу 1,
  иначе вернём множество разделителей.
\end{enumerate}

\subsubsection{ОМП-вывод}

ОМП для переключающейся скрытой марковской модели практически полностью
идентична ОМП для классической скрытой марковской модели. Кратко опишем отличающиеся
части шагов E и М.

\paragraph{Шаг E}

Изменим алгоритм прямого-обратного хода для переключающейся скрытой
марковской модели:
\begin{align*}
  \alpha_i(1)
  &= \pi_i \op{Bin}(k_1; n_1, p_i) \\
  \alpha_i(t)
  &= \left( \sum\limits_{j = 1}^S \alpha_j(t - 1) \mathbf{A}_{\op{w}(d_t)ji}
     \right) \op{Bin}(k_t; n_t, p_i) \\
  \beta_i(T)
  &= 1 \\
  \beta_i(t)
  &= \sum\limits_{j = 1}^S
     \mathbf{A}_{\op{w}(d_{t+1})ij} \op{Bin}(k_{t + 1}; n_{t + 1}, p_j) \beta_j(t + 1),
\end{align*}
формулы для вычисления $\gamma_{ti}$ и $\xi_{tij}$ при этом не изменятся.

\paragraph{Шаг М}

Воспользуемся методом множителей Лагранжа для учёта ограничений на матрицу перехода.
Составим функцию Лагранжа
\begin{align*}
  L(\mathbf{\theta}, \mathbf{\Lambda}|\mathbf{\theta}^{\op{old}})
  &= \sum\limits_{t = 2}^T
     \sum\limits_{i = 1}^S \sum\limits_{i = j}^S
     \E{\mathbf{s}|\mathbf{n}, \mathbf{k}, \mathbf{\theta}^{\op{old}}}
       {\I{s_{t - 1} = i}\I{s_t = j}} \log \mathbf{A}_{\op{w}(d_t)ij} \\
  &+ Q(\mathbf{\pi}, \mathbf{p}|\mathbf{\theta}^{\op{old}})
   + \sum\limits_{d = 1}^D \sum\limits_{i = 1}
     \mathbf{\Lambda}_{di} (1 - \sum\limits_{j = 1}^S \mathbf{A}_{dij}),
\end{align*}
где $Q(\mathbf{\pi}, \mathbf{p}|\mathbf{\theta}^{\op{old}})$ --- часть
функции $Q$, не зависящая от $\mathbf{A}$ (см. \cite[c. 620]{murphy2012machine}).

Обозначим
$$
\E{\mathbf{s}|\mathbf{n}, \mathbf{k}, \mathbf{\theta}^{\op{old}}}
  {\I{s_{t - 1} = i}\I{s_t = j}} := \xi_{tij}
$$
и найдём частные производные функции Лагранжа по $\mathbf{A}_{dij}$ и $\mathbf{\Lambda}_{di}$:
\begin{align*}
  \frac{\partial}{\partial \mathbf{A}_{dij}}
  L(\mathbf{\theta}, \mathbf{\Lambda}|\mathbf{\theta}^{\op{old}})
  &= \frac{1}{\mathbf{A}_{dij}} \sum\limits_{t = 2}^T
     \xi_{tij}\I{\op{w}(d_t) = d} - \mathbf{\Lambda}_{di} \equiv 0 \\
  &\Leftrightarrow \mathbf{A}_{dij}
   = \frac{1}{\mathbf{\Lambda}_{di}} \sum\limits_{t = 2}^T
     \xi_{tij} \I{\op{w}(d_t) = d} \\
  \frac{\partial}{\partial \mathbf{\Lambda}_{di}}
  L(\mathbf{\theta}, \mathbf{\Lambda}|\mathbf{\theta}^{\op{old}})
  &= 1 - \sum\limits_{j = 1}^S \mathbf{A}_{dij}
   = 1 -
     \frac{1}{\mathbf{\Lambda}_{di}} \sum\limits_{t = 2}^T
     \sum\limits_{j = 1}^S \xi_{tij} \I{\op{w}(d_t) = d} \equiv 0 \\
  &\Leftrightarrow \mathbf{\Lambda}_{di}
  = \sum\limits_{t = 2}^T
    \sum\limits_{j = 1}^S \xi_{tij} \I{\op{w}(d_t) = d} \\
  &\Rightarrow \mathbf{A}_{dij}^{\op{new}}
  = \frac{\sum\limits_{t = 2}^T \xi_{tij} \I{\op{w}(d_t) = d}}
         {\sum\limits_{j = 1}^S
          \sum\limits_{t = 2}^T \xi_{tij} \I{\op{w}(d_t) = d}}
\end{align*}

\subsubsection{Применение}

Переключающаяся биномиальная СММ обучается на данных бисульфитного секвенирования
первой хромосомы стволовых клеток мыши (см.~\ref{sub:data}). В качестве начального
приближения параметров выбираются параметры биномиальной смеси, обученной на тех же
данных (см.~\ref{bhmm:applications}). Кластеризация расстояний производилась с
помощью алгоритма <<голова/хвост>>. В таблице~\ref{tab:trans} приведены полученные в
результате обучения вероятности перехода для трех кластеров расстояний. Можно заметить,
что в матрицах перехода, соответствующих небольшим расстояниям (кластеры 1, 5),
наибольшие вероятности расположены на главной диагонали. В матрице для удаленных
цитозинов (кластер 9) наблюдается обратная ситуация. То есть близко расположенные
цитозины имеют значительно больший шанс оказаться в одном состоянии, чем удалённые.
Таким образом, предположение о влиянии расстояния между цитозинами на характер зависимости
между их состояниями подтвердилось.

\begin{table}[htbp!]
  \centering
  \begin{subtable}{.3\textwidth}
    \centering
    \begin{tabular}{lll}
      0.990 & 0.002 & 0.008 \\
      0.016 & 0.980 & 0.004 \\
      0.007 & 0.05 & 0.988
    \end{tabular}
    \caption{$\op{w}(d_t) = 1, d_t \in [1, 2]$}
  \end{subtable}\hspace{.025\textwidth}%
  \begin{subtable}{.3\textwidth}
    \centering
    \begin{tabular}{lll}
      0.966 & 0.020 & 0.014 \\
      0.021 & 0.976 & 0.003 \\
      0.006 & 0.007 & 0.987
    \end{tabular}
    \caption{$\op{w}(d_t) = 5, d_t \in [6, 9]$}
  \end{subtable}\hspace{.025\textwidth}%
  \begin{subtable}{.3\textwidth}
    \centering
    \begin{tabular}{lll}
      0.285 & 0.257 & 0.458 \\
      0.056 & 0.413 & 0.531 \\
      0.01  & 0.056 & 0.934
    \end{tabular}
    \caption{$\op{w}(d_t) = 9, d_t \ge 118$}
  \end{subtable}

  \caption{Матрицы вероятностей перехода для переключающейся биномиальной СММ.
    Метки столбцов и строчек слева-направо и сверху вниз: \texttt{UNMETHYLATED},
    \texttt{LOW}, \texttt{HIGH}.}
  \label{tab:trans}
\end{table}

Из графика сравнения классической и переключающейся биномиальной СММ
(рис.~\ref{fig:bhmm-bshmm}) видно, что переход к переключающейся модели улучшил
правдоподобие модели и значение критерия Акаике.

\begin{figure}[htbp!]
  \centering
  \includegraphics[width=\textwidth]{images/bhmm-bshmm}
  \caption{Сравнение биномиальной классической (BHMM) и переключающейся (BSHMM)
    скрытых марковских моделей на данных бисульфитного секвенирования
    стволовых клеток мыши.}
  \label{fig:bhmm-bshmm}
\end{figure}


\subsection{Переключающаяся мультиномиальная СММ}

\subsubsection{Мотивация}

В предыдущих рассуждениях мы исходили из того, что в прочтениях, полученных
в результате бисульфитного секвенирования, на месте цитозина может быть либо
цитозин, подтверждающий метилирование, либо тимин в обратом случае.
В реальных данных это предположение почти никогда не выполняется, так как
\begin{itemize}
\item в образце могла произойти точечная мутация, в результате которой
  цитозин заменился на другой нуклеотид,
\item секвенаторы делают ошибки при чтении фрагментов \cite{minoche2011evaluation}.
\end{itemize}

Для учёта таких позиций мы будем представлять результаты бисульфитного секвенирования
в виде вектора $x_t = \mathbf{c}_t$, где $c_{t1}$ --- количество прочтений, подтверждающих
аденин в данной позиции, $c_{t2}$ --- цитозин, $c_{t3}$ --- тимин и $c_{t4}$ --- гуанин.
Будем считать, что вектор $\mathbf{c}_t$ подчиняется мультиномиальному распределению с параметрами
$n_t = \sum\limits_{k = 1}^4 c_{tk}$ и $\mathbf{b} = (b_1, \ldots, b_4)$.

\subsubsection{Определение}

Пусть, как и прежде, $\mathbf{\pi}$, $\mathbf{A}$ --- параметры переключающейся
скрытой марковской модели, $\mathbf{B} = (\mathbf{b}_1, \ldots, \mathbf{b}_S)$
--- параметры мультиномиального распределения для каждого из состояний и
$\op{w}$ --- функция, кластеризующая расстояния, тогда функцию правдоподобия можно
записать как
$$
P(\mathbf{k}; \mathbf{n}, \mathbf{\pi}, \mathbf{A}, \mathbf{B})
= \sum\limits_{\mathbf{s} \in \{1, \ldots, S\}^T}
  \pi_{s_t}
  \prod\limits_{t = 2}^T A_{\op{w}(d_t) s_t s_{t - 1}}
  \prod\limits_{t = 1}^T \op{Multin}\left(\mathbf{c}_t; n_t, \mathbf{b}_{s_t}\right).
$$

\subsubsection{ОМП-вывод}

Новое приближение параметров мультиномиального распределение находится по
формулам:
\begin{align*}
  \mathbf{b}^{\op{new}}_{ik}
  = \frac{\sum\limits_{t = 1}^T \gamma_{ti} c_{tk}} {\sum\limits_{t = 1}^T \gamma_{ti} n_t}.
\end{align*}


\subsubsection{Применение}

Переключающаяся мультиномиальная СММ обучается на данных бисульфитного секвенирования
первой хромосомы стволовых клеток мыши (см.~\ref{sub:data}). Начальное приближение
для параметров модели находится с помощью уже упомянутого алгоритма кластеризации
KMeans++ (см.~\ref{bm:applications}). Из графика (рис.~\ref{fig:multinomial}) видно,
что предположение об ошибках секвенатора подтвердилось: в состоянии \texttt{UNMETHYLATED}
вероятности аденина и гуанина отличны от нуля.

%% TODO: объяснить почему мы не можем сравнивать правдоподобие с перелючающейся биномиальной СММ.
%% TODO: написать что-то про перелесть мультиномиальной модели по сравнению с биномиальной
%% в ключе учёта ошибок и потенциала для future work.

\FloatBarrier
\begin{figure}[h]
  \centering
  \includegraphics[width=.8\textwidth]{images/multinomial}
  \caption{Распределение нуклеотидов в каждом из состояний модели}
  \label{fig:multinomial}
\end{figure}
\FloatBarrier


\subsection{Программная реализация моделей}
\label{sub:implementation}

Модели были реализованы на языке Java в рамках биоинформатического проекта компании JetBrains.
Оценки времени работы моделей на компьютере с процессором Intel Core i5 1700Mhz и 8Гб оперативной
памяти приведены в таблице~\ref{tab:time}. Можно видеть, что с увеличением сложности модели время
работы увеличивается, но даже для переключающейся мультиномиальной СММ оно не превосходит
25 минут.

\begin{table}[htbp!]
  \centering
  \begin{tabular}{lr}
    \textbf{Модель} & \textbf{Время работы} \\
    \noalign{\nobreak\smallskip}
    Биномиальная смесь & 4 мин. 8 с. \\
    Биномиальная СММ & 6 мин. 55 с. \\
    Переключающаяся биномиальная СММ & 9 мин. 40 с. \\
    Переключающаяся мультиномиальная СММ & 22 мин. 25 с. \\
  \end{tabular}
  \caption{Время работы предлагаемых моделей на данных бисульфитного секвенирования
    первой хромосомы стволовых клеток мыши.}
  \label{tab:time}
\end{table}
